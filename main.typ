#set text(lang: "ru")

= Как нейросети могут помочь в обучении?

== Цели доклада:

+ Показать историю развития нейронных сетей.

+ Объяснить принципы работы современных нейросетей.

+ Обсудить преимущества и ограничения использования нейросетей в образовании.

+ Предложить конкретные способы интеграции нейросетей в учебный процесс.

== План доклада:

=== Часть I. История развития нейронных сетей

- *Концептуальные основы* _(XIX век)_ -- Работы Германа фон Гельмгольца и Чарльза Шеррингтона.

- *Первые модели искусственных нейронов* _(XX век)_ -- Перцептроны Розенблатта _(1958)_. Многослойные перцептроны и алгоритм обратного распространения ошибок _(1980-е годы)_.

- *Современные достижения* -- Глубокое обучение (Deep Learning). Архитектуры сверточных нейронных сетей (CNN), рекуррентных нейронных сетей (RNN), трансформеры.

=== Часть II. Принципы работы нейросетей

- *Структура нейронной сети* -- Входной слой, скрытые слои, выходной слой. Весовые коэффициенты и активации.

- *Алгоритмы обучения* -- Обратное распространение ошибки. Оптимизационные методы (SGD, Adam).

- *Примеры успешных приложений* -- Распознавание изображений и речи. Генерация текста и диалоговых агентов.

=== Часть III. Нейросети в образовательном процессе

- *Индивидуализация обучения* -- Анализ успеваемости учащихся. Создание персонализированных учебных планов.

- *Автоматизированная проверка заданий* -- Оценка письменных работ. Проверка решений тестов и экзаменов.

- *Интерактивные образовательные инструменты* -- Чат-боты для помощи студентам. Виртуальные ассистенты преподавателя.

=== Заключение

Обобщение преимуществ и ограничений использования нейросетей в обучении. Подведение итогов о потенциале технологии для повышения качества образования.

#pagebreak()

= История развития нейронных сетей
//
// Часть 1.1
//
== Концептуальные основы

Концептуальные основы нейрофизиологии _XIX_ века во многом связаны с работами Германа фон Гельмгольца и Чарльза Шеррингтона. Их идеи заложили фундамент современной нейронауки, физиологии и понимания работы нервной системы.

=== Герман фон Гельмгольц _(1821 -- 1894)_

#figure(
  image("images/img1.jpg", width: 63%),
)

Герман фон Гельмгольц был одним из самых влиятельных учёных _XIX_ века. Он работал на стыке *физики, физиологии и медицины*, что позволило ему применять строгие количественные методы к изучению живых систем. Его исследования по нервной системе и восприятию стали фундаментом для понимания работы мозга и, косвенно, для создания моделей искусственных нейронов.

==== Контекст научной эпохи

В первой половине _XIX_ века нервная система рассматривалась скорее философски, чем научно. Многие считали, что нервные сигналы передаются мгновенно и что процесс мышления и ощущения не поддаётся измерению. Существовало представление о "животной силе" -- некой мистической энергии, которая движет мышцами и органами чувств.

Гельмгольц поставил себе задачу объективно измерить нервные процессы, применяя методы физики. Он считал, что биологические процессы могут быть количественно описаны так же, как и процессы в механике или оптике. Это был революционный подход: идея, что нервы -- это проводники сигналов с конечной скоростью, стала ключевым шагом к современной нейрофизиологии.

==== Измерение скорости нервного импульса

Одним из главных достижений Гельмгольца было *экспериментальное измерение скорости распространения нервного импульса*. До него многие считали, что нервный сигнал распространяется мгновенно. Гельмгольц использовал электрические и механические приборы для фиксации времени, которое требуется нерву, чтобы передать сигнал от точки А к точке Б.

Пример эксперимента:

 - Он стимулировал нерв в ноге лягушки электрическим разрядом.
 - Измерял время между стимулом и сокращением мышцы с помощью *механического телеграфного устройства*.
 - Полученные значения показали скорость около 30-40 м/с, что намного меньше скорости света, но достаточно быстро для координации движений организма.

Этот эксперимент имел два ключевых следствия:

 + Нервные процессы имеют *конкретные количественные параметры*, которые можно измерять и моделировать.
 + Нервная система работает как *информационная сеть*, где сигналы проходят по проводникам и могут быть замедлены или ускорены.

Для будущих разработчиков искусственных нейронов это было критически важно: сигнал в нейроне не мгновенен, его передача зависит от характеристик соединений (синапсов) и времени обработки

=== Гельмгольц и теория чувств

Помимо скорости нервных сигналов, Гельмгольц занимался изучением *чувств и восприятия*. Он разработал *теорию зрения и слуха*, которая объясняла, как физические стимулы превращаются в нервные сигналы:

 + Цветовое зрение и теория трихроматии
    - *Гельмгольц* развил идеи Томаса Юнга о трёх видах цветовых рецепторов в сетчатке глаза (красный, зелёный, синий).
    - Он показал, что человеческое восприятие цвета может быть *смоделировано математически*, что дало основу для обработки сигналов и «кодирования» информации в нейросетях.
 + Акустика и слух
    - Исследовал восприятие звука, высоту и громкость.
    - Разработал *тонометр*, который позволял измерять реакцию слухового нерва на звуковые частоты.

Эти работы показали, что нервная система не просто передаёт сигналы, а *кодирует и обрабатывает информацию*, что напрямую вдохновило концепцию искусственных нейронов, которые тоже преобразуют входные сигналы в выходные через активационные функции.

==== Заключение
 + *скорость нервного импульса* -- Гельмгольц *впервые* измерил, что нервные сигналы передаются не мгновенно, а с конечной скоростью (~30-40 м/с), показав, что нервная система работает как информационная сеть.

 + *Математическое описание нервов* -- Он применял физику и математику к биологии, моделируя нерв как проводник сигналов с задержками и суммированием, закладывая основу количественного подхода к нейронам.

 + *Теория чувств и восприятия* -- Изучал зрение и слух, показав, что сенсорные сигналы кодируются и могут быть количественно описаны, что вдохновило идеи обработки информации в искусственных нейронных сетях.

=== Чарльз Шеррингтон _(1857-1952)_

#figure(
  image("images/img2.jpg", width: 50%)
)

*Чарльз Шеррингтон* -- выдающийся британский физиолог, который сыграл *ключевую роль* в понимании работы нервной системы. Его исследования легли в основу концепции *нейронной сети*, хотя в его время компьютеров ещё не существовало. Шеррингтон систематизировал знания о нервных соединениях, рефлексах и взаимодействии нейронов, что позволило представить нервную систему как *информационную сеть с обработкой сигналов*.

==== Исследование синапсов

Одним из *главных* достижений Шеррингтона стало введение понятия синапса.

 - *Синапс* -- это место контакта между двумя нейронами, где передается нервный сигнал.
 - Шеррингтон показал, что сигнал не проходит напрямую, а может быть возбуждающим или тормозящим, в зависимости от типа синапса.
 - Он доказал, что нервная система функционирует через множество взаимодействующих точек связи, что создает сложную системы обработки информации.

Синапсы в мозге вдохновили разработку входных весов в искусственных нейронах, которые регулируют силу и эффект передаваемого сигнала.

==== Принцип интеграции сигналов

*Шеррингтон* показал, что нейрон не реагирует на одиночный сигнал. Он активируется только после суммирования нескольких входных импульсов, что он назвал постсинаптической интеграцией.

 - Сигналы могут поступать с разных нейронов одновременно или с небольшими задержками.
 - Нейрон оценивает совокупность этих сигналов и решает, передавать ли сигнал дальше.
 - Это важный механизм фильтрации информации, предотвращающий "шумиху" от случайных возбуждений.

Принцип интеграции сигналов -- прямой аналог суммирования взвешенных входов в искусственном нейроне перед активацией.

==== Рефлекторные дуги и обратная связь

*Шеррингтон* также подробно изучал рефлексы: автоматические реакции на стимулы.

 - Он описал рефлекторную дугу -- путь, по которому сенсорный сигнал вызывает моторный ответ.
 - Эти дуги демонстрировали обратную связь и координацию между различными участками нервной системы.
 - Рефлексы показывают, что нервная система не пассивна; она обрабатывает и реагирует на информацию динамически.

Идея обратной связи вдохновила методы обучения нейросетей, в частности backpropagation, когда ошибка на выходе сети корректирует веса синапсов (аналог влияния сигналов на реальный нейрон).

==== Влияние на искусственные нейронные сети

 + Синапсы -- веса нейронов
    - Концепция Шеррингтона о синапсах стала прямым аналогом весов в искусственных нейронах.
    - Сила синапса определяет, будет ли сигнал возбуждать или тормозить следующий нейрон, как и вес в модели.

 + Суммирование сигналов -- активация нейрона
    - Принцип интеграции сигналов объясняет, почему нейрон активируется только при определённой совокупности входов.
    - Искусственный нейрон выполняет то же суммирование взвешенных входов перед применением функции активации.

 + Рефлексы и обратная связь -- обучение сети
    - Механизмы обратной связи в нервной системе вдохновили алгоритмы обучения нейросетей, где ошибка корректирует веса для улучшения результата.

==== Заключение по вкладу Шеррингтона

Чарльз Шеррингтон сделал переход от абстрактного понятия нервной системы к *конкретным структурным и функциональным механизмам*:

 - Ввёл понятие синапса, объяснив, как нейроны взаимодействуют.
 - Показал, что нейрон суммирует сигналы и активируется только при достижении порога.
 - Исследовал рефлексы, выявив принципы обратной связи и динамической обработки информации.

Его открытия стали биологической основой для конструкции *искусственных нейронных сетей*, заложив принципы суммирования, взвешивания и обратной связи, которые применяются во всех современных нейросетевых моделях.
//
// часть 1.2
//
== Первые модели искусственных нейронов

==== Перцептроны Розенблатта _(1958)_

Фрэнк Розенблатт в _1958_ году предложил первую практическую модель искусственного нейрона, которую он назвал перцептроном. Основные идеи:

 + Принцип работы
    - Перцептрон имитировал один нейрон: получал несколько входных сигналов, каждый из которых умножался на «вес».
    - Сумма взвешенных входов подавалась на функцию активации (например, пороговую), которая определяла, будет ли нейрон «срабатывать».

 + Обучение перцептрона
    - Розенблатт предложил простое правило корректировки весов, основанное на ошибке между желаемым и фактическим выходом.
    - Если выход был неверным, веса корректировались пропорционально входам, чтобы нейрон учился различать примеры.

 + Возможности и ограничения
   - Перцептроны могли решать линейно разделимые задачи (например, классификацию по простым линиям в 2D пространстве).
   - Однако они не справлялись с нелинейно разделимыми задачами, такими как XOR. Это ограничение долгое время замедляло развитие нейросетей.

Розенблатт создал *первый* искусственный нейрон, который можно обучать на основе примеров, и продемонстрировал принцип "вход - взвешивание - активация - корректировка".

==== Многослойные перцептроны _(1980-е годы)_

В _1980-е_ годы нейросети пережили второе рождение благодаря многослойным перцептронам (MLP) и алгоритму обратного распространения ошибок (backpropagation):

 + Многослойная структура
    - Вместо одного слоя нейронов вводились скрытые слои, которые позволяли сети моделировать сложные нелинейные зависимости.
    - Каждый нейрон скрытого слоя выполнял ту же функцию, что и перцептрон: суммировал входы и применял активацию.

 + Алгоритм обратного распространения ошибок
    - Метод позволяет обучать сеть с несколькими слоями: ошибка на выходе распространяется назад по сети, корректируя веса каждого нейрона.
    - Это принципиально решало проблему нелинейно разделимых задач, таких как XOR, которые перцептроны Розенблатта не могли решить.

 + Влияние на развитие ИИ
    - Многослойные сети с backpropagation стали основой современных глубоких нейросетей.
    - Они позволяли моделировать сложные функции, распознавать образы, речь и текст.

Основная идея: многослойность и обратное распространение ошибок сделали нейросети способными решать практически любые сложные задачи.

==== Связь с биологией

 - Концепции синапсов и суммирования сигналов из работ Шеррингтона стали прототипом входных весов и функций активации.

 - Идея обратной связи в обучении от рефлексов и механизмов коррекции сигналов вдохновила обратное распространение ошибок.

 - Таким образом, теория нейрофизиологии Гельмгольца и Шеррингтона напрямую повлияла на построение искусственных нейронных сетей, хотя формально это произошло почти через 100 лет.
 //
 // часть 1.3
 //
== Глубокое обучение (Deep Learning) [L217]

=== Архитектуры глубоких нейронных сетей [L219]

Глубокое обучение (Deep Learning) -- это подмножество машинного обучения, которое использует нейронные сети с множеством слоёв для обработки и анализа сложных данных. В последние два десятилетия глубокое обучение произвело революцию в компьютерном зрении, обработке естественного языка и многих других областях.

==== Сверточные нейронные сети (CNN -- Convolutional Neural Networks) [L223]

===== Основные принципы [L225]

Сверточные нейронные сети разработаны специально для обработки данных с сеточной структурой, особенно изображений. Ключевые компоненты CNN:

+ *Свёртка (Convolution):* операция, при которой фильтр (ядро) скользит по входному изображению, вычисляя скалярное произведение с локальными участками. Это позволяет извлекать локальные признаки.

+ *Пулинг (Pooling):* операция уменьшения размерности, которая берёт максимальное или среднее значение из локального окна. Пулинг снижает объём вычислений и повышает инвариантность к небольшим смещениям.

+ *Нелинейная активация:* использование функций активации (ReLU, сигмоида) между слоями для введения нелинейности.

+ *Полносвязные слои:* в конце сети обычно располагаются полносвязные слои для классификации или регрессии.

===== Архитектура и примеры [L239]

Типичная архитектура CNN состоит из чередующихся слоёв свёртки, активации и пулинга, за которыми следуют полносвязные слои:

```
Входное изображение → Свёртка + ReLU → Пулинг → Свёртка + ReLU → Пулинг → Полносвязный слой → Выход
```

Известные архитектуры включают:

- *LeNet-5* (Йан ЛеКун, 1998) -- одна из первых успешных CNN для распознавания рукописных цифр.
- *AlexNet* (2012) -- глубокая сеть, выигравшая конкурс ImageNet, положила начало современной эре глубокого обучения.
- *VGG* (2014) -- демонстрирует важность глубины сети.
- *ResNet* (2015) -- внедрила остаточные связи (skip connections) для обучения очень глубоких сетей.
- *Inception/GoogLeNet* -- использует параллельные сверточные фильтры разных размеров.

===== Применение [L257]

CNN особенно успешны в:

- Классификации изображений (распознавание объектов)
- Обнаружении объектов (YOLO, Faster R-CNN)
- Семантической сегментации
- Распознавании лиц и биометрии
- Медицинской диагностике (анализ рентгеновских снимков и КТ)

==== Рекуррентные нейронные сети (RNN -- Recurrent Neural Networks) [L267]

===== Основные принципы [L269]

Рекуррентные нейронные сети предназначены для обработки последовательностей данных, где порядок элементов важен. Ключевая характеристика RNN -- наличие обратных связей, которые передают информацию из предыдущих временных шагов в текущий.

Базовая формула RNN:

```
h_t = tanh(W_hh * h_(t-1) + W_xh * x_t + b_h)
y_t = W_hy * h_t + b_y
```

где `h_t` -- скрытое состояние, `x_t` -- входные данные в момент времени t.

===== Архитектуры RNN [L283]

+ *Ванильные RNN:* простые рекуррентные сети, но страдают от проблемы исчезающего градиента при длинных последовательностях.

+ *LSTM (Long Short-Term Memory):* разработаны для решения проблемы исчезающего градиента. Включают ячейки памяти и три типа вентилей:
  - Входной вентиль -- контролирует, какая информация попадает в ячейку
  - Вентиль забывания -- контролирует, какую информацию удалять
  - Выходной вентиль -- контролирует, какую информацию выводить

+ *GRU (Gated Recurrent Unit):* упрощённая версия LSTM с двумя вентилями вместо трёх.

===== Применение [L299]

RNN и их варианты успешно применяются в:

- Обработке естественного языка (NLP) -- машинный перевод, генерация текста
- Распознавании речи
- Анализе временных рядов и прогнозировании
- Музыкальной композиции
- Автозаполнении текста и автокоррекции
- Видеоанализе (обработка последовательности кадров)

==== Трансформеры (Transformers) [L309]

===== Революция механизма внимания [L311]

Трансформеры, предложенные в статье "Attention is All You Need" (Васвани и др., 2017), кардинально изменили подход к обработке последовательностей. Вместо рекуррентных связей они используют механизм *самовнимания (self-attention)*, который позволяет модели напрямую сравнивать расстояния между всеми элементами последовательности.

Основная идея самовнимания:

+ Преобразование входной последовательности в три компоненты: запросы (Query), ключи (Key) и значения (Value).
+ Вычисление оценок внимания как скалярного произведения между запросами и ключами.
+ Масштабирование оценок и применение софтмакса для получения весов внимания.
+ Взвешенное суммирование значений по полученным весам.

===== Архитектура трансформера [L327]

Трансформер состоит из:

+ *Кодировщик (Encoder):* стопка блоков, каждый из которых содержит:
  - Многоголовый механизм самовнимания
  - Полносвязную прямую нейросеть
  - Нормализация слоёв и остаточные связи

+ *Декодировщик (Decoder):* аналогичная структура плюс дополнительный механизм внимания между выходом кодировщика и текущим уровнем декодировщика.

+ *Позиционные кодирования:* так как трансформер не имеет встроенного понимания порядка, добавляются синусоидальные кодирования позиций.

==== Преимущества перед RNN [L341]

- *Параллелизм:* все элементы последовательности обрабатываются параллельно, что ускоряет вычисления.
- *Долгосрочные зависимости:* механизм внимания лучше захватывает долгосрочные зависимости.
- *Масштабируемость:* трансформеры хорошо масштабируются на больших объёмах данных.

===== Современные модели на основе трансформеров [L348]

- *BERT* (Bidirectional Encoder Representations from Transformers) -- предварительно обученная модель для понимания текста.
- *GPT серия* (Generative Pre-trained Transformer) -- большие языковые модели для генерации текста, включая ChatGPT.
- *Vision Transformer (ViT)* -- применение трансформеров к компьютерному зрению.
- *T5* -- унифицированная архитектура для различных NLP задач.
- *DALL-E, Stable Diffusion* -- применение в генерации изображений.

===== Применение [L358]

Трансформеры преобразили следующие области:

- Машинный перевод (нейронный машинный перевод высокого качества)
- Генерация текста и диалоговые системы
- Анализ тональности и классификация текста
- Вопросно-ответные системы
- Генерация кода и программирование с помощью ИИ
- Генерация и редактирование изображений
- Музыка и другие творческие задачи

=== Сравнение архитектур [L352]

#table(
  columns: (1fr, 1fr, 1fr, 1fr),
  [*Характеристика*], [*CNN*], [*RNN/LSTM*], [*Трансформер*],
  [Входные данные], [Сеточные структуры], [Последовательности], [Последовательности],
  [Параллелизм], [Высокий], [Низкий], [Высокий],
  [Долгосрочные зависимости], [Локальные], [Проблемы (LSTM улучшает)], [Отличные],
  [Эффективность памяти], [Средняя], [Экономная], [Требует много памяти],
  [Основное применение], [Изображения], [Последовательности, NLP], [NLP, мультимодальные данные],
)

=== Выводы [L382]

Развитие архитектур глубоких нейронных сетей -- от CNN для компьютерного зрения, через RNN для обработки последовательностей, к трансформерам для универсальной обработки различных типов данных -- отражает постоянное совершенствование методов извлечения и обработки информации. Каждая архитектура имеет свои сильные стороны и остаётся актуальной в своих областях применения. Современные тренды показывают движение к более универсальным архитектурам и комбинированию различных подходов для решения сложных задач в образовании, науке и промышленности.
