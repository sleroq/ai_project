#set text(lang: "ru")

= Как нейросети могут помочь в обучении?
/*
== Цели доклада:

+ Показать историю развития нейронных сетей.

+ Объяснить принципы работы современных нейросетей.

+ Обсудить преимущества и ограничения использования нейросетей в образовании.

+ Предложить конкретные способы интеграции нейросетей в учебный процесс.

== План доклада:

=== Часть I. История развития нейронных сетей

- *Концептуальные основы* _(XIX век)_ -- Работы Германа фон Гельмгольца и Чарльза Шеррингтона.

- *Первые модели искусственных нейронов* _(XX век)_ -- Перцептроны Розенблатта _(1958)_. Многослойные перцептроны и алгоритм обратного распространения ошибок _(1980-е годы)_.

- *Современные достижения* -- Глубокое обучение (Deep Learning). Архитектуры сверточных нейронных сетей (CNN), рекуррентных нейронных сетей (RNN), трансформеры.

=== Часть II. Принципы работы нейросетей

- *Структура нейронной сети* -- Входной слой, скрытые слои, выходной слой. Весовые коэффициенты и активации.

- *Алгоритмы обучения* -- Обратное распространение ошибки. Оптимизационные методы (SGD, Adam).

- *Примеры успешных приложений* -- Распознавание изображений и речи. Генерация текста и диалоговых агентов.

=== Часть III. Нейросети в образовательном процессе

- *Индивидуализация обучения* -- Анализ успеваемости учащихся. Создание персонализированных учебных планов.

- *Автоматизированная проверка заданий* -- Оценка письменных работ. Проверка решений тестов и экзаменов.

- *Интерактивные образовательные инструменты* -- Чат-боты для помощи студентам. Виртуальные ассистенты преподавателя.

=== Заключение

Обобщение преимуществ и ограничений использования нейросетей в обучении. Подведение итогов о потенциале технологии для повышения качества образования.

#pagebreak()
*/
= История развития нейронных сетей
//
// Часть 1.1
//
== Концептуальные основы

Концептуальные основы нейрофизиологии _XIX_ века во многом связаны с работами Германа фон Гельмгольца и Чарльза Шеррингтона. Их идеи заложили фундамент современной нейронауки, физиологии и понимания работы нервной системы.

=== Герман фон Гельмгольц _(1821 -- 1894)_

#figure(
  image("images/img1.jpg", width: 63%),
)

Герман фон Гельмгольц был одним из самых влиятельных учёных _XIX_ века. Он работал на стыке *физики, физиологии и медицины*, что позволило ему применять строгие количественные методы к изучению живых систем. Его исследования по нервной системе и восприятию стали фундаментом для понимания работы мозга и, косвенно, для создания моделей искусственных нейронов.

==== Контекст научной эпохи

В первой половине _XIX_ века нервная система рассматривалась скорее философски, чем научно. Многие считали, что нервные сигналы передаются мгновенно и что процесс мышления и ощущения не поддаётся измерению. Существовало представление о "животной силе" -- некой мистической энергии, которая движет мышцами и органами чувств.

Гельмгольц поставил себе задачу объективно измерить нервные процессы, применяя методы физики. Он считал, что биологические процессы могут быть количественно описаны так же, как и процессы в механике или оптике. Это был революционный подход: идея, что нервы -- это проводники сигналов с конечной скоростью, стала ключевым шагом к современной нейрофизиологии.

==== Измерение скорости нервного импульса

Одним из главных достижений Гельмгольца было *экспериментальное измерение скорости распространения нервного импульса*. До него многие считали, что нервный сигнал распространяется мгновенно. Гельмгольц использовал электрические и механические приборы для фиксации времени, которое требуется нерву, чтобы передать сигнал от точки А к точке Б.

Пример эксперимента:

 - Он стимулировал нерв в ноге лягушки электрическим разрядом.
 - Измерял время между стимулом и сокращением мышцы с помощью *механического телеграфного устройства*.
 - Полученные значения показали скорость около 30-40 м/с, что намного меньше скорости света, но достаточно быстро для координации движений организма.

Этот эксперимент имел два ключевых следствия:

 + Нервные процессы имеют *конкретные количественные параметры*, которые можно измерять и моделировать.
 + Нервная система работает как *информационная сеть*, где сигналы проходят по проводникам и могут быть замедлены или ускорены.

Для будущих разработчиков искусственных нейронов это было критически важно: сигнал в нейроне не мгновенен, его передача зависит от характеристик соединений (синапсов) и времени обработки

=== Гельмгольц и теория чувств

Помимо скорости нервных сигналов, Гельмгольц занимался изучением *чувств и восприятия*. Он разработал *теорию зрения и слуха*, которая объясняла, как физические стимулы превращаются в нервные сигналы:

 + Цветовое зрение и теория трихроматии
    - *Гельмгольц* развил идеи Томаса Юнга о трёх видах цветовых рецепторов в сетчатке глаза (красный, зелёный, синий).
    - Он показал, что человеческое восприятие цвета может быть *смоделировано математически*, что дало основу для обработки сигналов и «кодирования» информации в нейросетях.
 + Акустика и слух
    - Исследовал восприятие звука, высоту и громкость.
    - Разработал *тонометр*, который позволял измерять реакцию слухового нерва на звуковые частоты.

Эти работы показали, что нервная система не просто передаёт сигналы, а *кодирует и обрабатывает информацию*, что напрямую вдохновило концепцию искусственных нейронов, которые тоже преобразуют входные сигналы в выходные через активационные функции.

==== Заключение
 + *скорость нервного импульса* -- Гельмгольц *впервые* измерил, что нервные сигналы передаются не мгновенно, а с конечной скоростью (~30-40 м/с), показав, что нервная система работает как информационная сеть.

 + *Математическое описание нервов* -- Он применял физику и математику к биологии, моделируя нерв как проводник сигналов с задержками и суммированием, закладывая основу количественного подхода к нейронам.

 + *Теория чувств и восприятия* -- Изучал зрение и слух, показав, что сенсорные сигналы кодируются и могут быть количественно описаны, что вдохновило идеи обработки информации в искусственных нейронных сетях.

=== Чарльз Шеррингтон _(1857-1952)_

#figure(
  image("images/img2.jpg", width: 50%)
)

*Чарльз Шеррингтон* -- выдающийся британский физиолог, который сыграл *ключевую роль* в понимании работы нервной системы. Его исследования легли в основу концепции *нейронной сети*, хотя в его время компьютеров ещё не существовало. Шеррингтон систематизировал знания о нервных соединениях, рефлексах и взаимодействии нейронов, что позволило представить нервную систему как *информационную сеть с обработкой сигналов*.

==== Исследование синапсов

Одним из *главных* достижений Шеррингтона стало введение понятия синапса.

 - *Синапс* -- это место контакта между двумя нейронами, где передается нервный сигнал.
 - Шеррингтон показал, что сигнал не проходит напрямую, а может быть возбуждающим или тормозящим, в зависимости от типа синапса.
 - Он доказал, что нервная система функционирует через множество взаимодействующих точек связи, что создает сложную системы обработки информации.

Синапсы в мозге вдохновили разработку входных весов в искусственных нейронах, которые регулируют силу и эффект передаваемого сигнала.

==== Принцип интеграции сигналов

*Шеррингтон* показал, что нейрон не реагирует на одиночный сигнал. Он активируется только после суммирования нескольких входных импульсов, что он назвал постсинаптической интеграцией.

 - Сигналы могут поступать с разных нейронов одновременно или с небольшими задержками.
 - Нейрон оценивает совокупность этих сигналов и решает, передавать ли сигнал дальше.
 - Это важный механизм фильтрации информации, предотвращающий "шумиху" от случайных возбуждений.

Принцип интеграции сигналов -- прямой аналог суммирования взвешенных входов в искусственном нейроне перед активацией.

==== Рефлекторные дуги и обратная связь

*Шеррингтон* также подробно изучал рефлексы: автоматические реакции на стимулы.

 - Он описал рефлекторную дугу -- путь, по которому сенсорный сигнал вызывает моторный ответ.
 - Эти дуги демонстрировали обратную связь и координацию между различными участками нервной системы.
 - Рефлексы показывают, что нервная система не пассивна; она обрабатывает и реагирует на информацию динамически.

Идея обратной связи вдохновила методы обучения нейросетей, в частности backpropagation, когда ошибка на выходе сети корректирует веса синапсов (аналог влияния сигналов на реальный нейрон).

==== Влияние на искусственные нейронные сети

 + Синапсы -- веса нейронов
    - Концепция Шеррингтона о синапсах стала прямым аналогом весов в искусственных нейронах.
    - Сила синапса определяет, будет ли сигнал возбуждать или тормозить следующий нейрон, как и вес в модели.

 + Суммирование сигналов -- активация нейрона
    - Принцип интеграции сигналов объясняет, почему нейрон активируется только при определённой совокупности входов.
    - Искусственный нейрон выполняет то же суммирование взвешенных входов перед применением функции активации.

 + Рефлексы и обратная связь -- обучение сети
    - Механизмы обратной связи в нервной системе вдохновили алгоритмы обучения нейросетей, где ошибка корректирует веса для улучшения результата.

==== Заключение по вкладу Шеррингтона

Чарльз Шеррингтон сделал переход от абстрактного понятия нервной системы к *конкретным структурным и функциональным механизмам*:

 - Ввёл понятие синапса, объяснив, как нейроны взаимодействуют.
 - Показал, что нейрон суммирует сигналы и активируется только при достижении порога.
 - Исследовал рефлексы, выявив принципы обратной связи и динамической обработки информации.

Его открытия стали биологической основой для конструкции *искусственных нейронных сетей*, заложив принципы суммирования, взвешивания и обратной связи, которые применяются во всех современных нейросетевых моделях.
//
// часть 1.2
//
== Первые модели искусственных нейронов

==== Перцептроны Розенблатта _(1958)_

Фрэнк Розенблатт в _1958_ году предложил первую практическую модель искусственного нейрона, которую он назвал перцептроном. Основные идеи:

 + Принцип работы
    - Перцептрон имитировал один нейрон: получал несколько входных сигналов, каждый из которых умножался на «вес».
    - Сумма взвешенных входов подавалась на функцию активации (например, пороговую), которая определяла, будет ли нейрон «срабатывать».

 + Обучение перцептрона
    - Розенблатт предложил простое правило корректировки весов, основанное на ошибке между желаемым и фактическим выходом.
    - Если выход был неверным, веса корректировались пропорционально входам, чтобы нейрон учился различать примеры.

 + Возможности и ограничения
   - Перцептроны могли решать линейно разделимые задачи (например, классификацию по простым линиям в 2D пространстве).
   - Однако они не справлялись с нелинейно разделимыми задачами, такими как XOR. Это ограничение долгое время замедляло развитие нейросетей.

Розенблатт создал *первый* искусственный нейрон, который можно обучать на основе примеров, и продемонстрировал принцип "вход - взвешивание - активация - корректировка".

==== Многослойные перцептроны _(1980-е годы)_

В _1980-е_ годы нейросети пережили второе рождение благодаря многослойным перцептронам (MLP) и алгоритму обратного распространения ошибок (backpropagation):

 + Многослойная структура
    - Вместо одного слоя нейронов вводились скрытые слои, которые позволяли сети моделировать сложные нелинейные зависимости.
    - Каждый нейрон скрытого слоя выполнял ту же функцию, что и перцептрон: суммировал входы и применял активацию.

 + Алгоритм обратного распространения ошибок
    - Метод позволяет обучать сеть с несколькими слоями: ошибка на выходе распространяется назад по сети, корректируя веса каждого нейрона.
    - Это принципиально решало проблему нелинейно разделимых задач, таких как XOR, которые перцептроны Розенблатта не могли решить.

 + Влияние на развитие ИИ
    - Многослойные сети с backpropagation стали основой современных глубоких нейросетей.
    - Они позволяли моделировать сложные функции, распознавать образы, речь и текст.

Основная идея: многослойность и обратное распространение ошибок сделали нейросети способными решать практически любые сложные задачи.

==== Связь с биологией

 - Концепции синапсов и суммирования сигналов из работ Шеррингтона стали прототипом входных весов и функций активации.

 - Идея обратной связи в обучении от рефлексов и механизмов коррекции сигналов вдохновила обратное распространение ошибок.

 - Таким образом, теория нейрофизиологии Гельмгольца и Шеррингтона напрямую повлияла на построение искусственных нейронных сетей, хотя формально это произошло почти через 100 лет.
 //
 // часть 1.3
 //
== Глубокое обучение (Deep Learning)

=== Архитектуры глубоких нейронных сетей

Глубокое обучение (Deep Learning) -- это способ обучения нейронных сетей с использованием множества слоёв. Представьте, что каждый слой сети учится всё более сложным вещам: первый слой видит простые линии, второй -- формы, третий -- целые объекты. В последние двадцать лет глубокое обучение начало помогать в распознавании фотографий, переводе текстов, создании голосовых помощников и даже в образовании -- например, при анализе ошибок учащихся.

==== Сверточные нейронные сети (CNN)

===== Основные принципы

Сверточные нейронные сети (CNN) специально созданы для работы с изображениями. Их главная идея -- сеть как бы скользит по изображению, смотря на маленькие окошки и выделяя из них важные признаки. Вот как это работает:

+ *Скользящий фильтр:* представьте, что у вас есть маленькая лупа, которая проходит по всему изображению и ищет в каждом месте узоры (линии, углы, текстуры).

+ *Уменьшение размера:* когда обнаружены основные признаки, сеть выбирает самые важные из них, чтобы не хранить слишком много информации.

+ *Активация:* между слоями добавляются простые "выключатели", которые говорят сети: "если это число маленькое, забудь о нём".

+ *Финальное решение:* в конце сеть собирает все найденные признаки и принимает решение (например, "это кот" или "это собака").

===== Архитектура и примеры

Сверточная сеть работает слой за слоем. На первых слоях сеть находит простые детали (края, углы), на следующих слоях комбинирует их в более сложные формы (колесо, ушо), а на последних слоях распознаёт целые объекты.

Несколько известных систем:

- *LeNet* (1998) -- первая система, которая научилась распознавать рукописные цифры. Почта использовала эту технологию для автоматической сортировки.
- *AlexNet* (2012) -- показала, что глубокие сети могут распознавать сложные картинки намного лучше, чем раньше.
- *Более новые системы* -- современные сети распознают не просто объекты, но и понимают, кто находится на фото, какое выражение лица, и даже могут помочь врачам найти болезнь на медицинских снимках.

===== Применение

Сверточные сети используются везде, где нужно понять изображения:

- *Распознавание объектов:* что изображено на фото (машина, дерево, человек)
- *Проверка качества в производстве:* находить бракованные детали на конвейере
- *Распознавание лиц:* разблокировка телефона по лицу, поиск людей на видео
- *Помощь врачам:* выявление опухолей на рентгеновских снимках
- *В образовании:* проверка почерка в письменных работах и распознавание того, что написал ученик на доске

Такие системы могут помочь учителям проверять работы быстрее и дать ученикам мгновенную обратную связь.

==== Рекуррентные нейронные сети (RNN)

===== Основные принципы

Рекуррентные нейронные сети (RNN) работают с текстом и звуком -- с информацией, которая идёт одна за другой, как слова в предложении. Главная идея RNN -- сеть помнит, что было раньше. Например, если вы читаете "кот сидит на...", вы уже ожидаете слово "диване" или "стуле", потому что помните контекст.

Так же работает RNN:
- Сеть смотрит на слово "кот" и запоминает эту информацию
- Потом смотрит на слово "сидит" и вспоминает, что было про кота
- На каждом шаге сеть использует информацию из прошлого, чтобы лучше понять текущее слово

===== Архитектуры RNN

Есть разные типы рекуррентных сетей:

+ *Простые RNN:* работают, но при работе с длинными текстами могут "забывать" начало предложения.

+ *LSTM (улучшенная версия):* как записная книжка с "вентилями" для управления памятью. Сеть может:
  - Впитать новую информацию (входной вентиль)
  - Удалить ненужное (вентиль забывания)
  - Вывести результат (выходной вентиль)

  Благодаря этому LSTM хорошо запоминает информацию даже в длинных текстах.

+ *GRU:* ещё более простая версия LSTM, которая работает немного быстрее.

===== Применение

RNN используются везде, где важен порядок информации:

- *Перевод текстов:* "Google Translate" использует RNN для перевода фраз с учётом контекста
- *Голосовые помощники:* Alexa и Siri распознают вашу речь с помощью RNN
- *Предсказание слова:* когда вы пишете СМС, ваш телефон предлагает следующее слово
- *Проверка орфографии:* система понимает, какое слово вы имели в виду
- *В образовании:* система может распознать ошибки в речи ученика при изучении иностранного языка и помочь исправить акцент

Благодаря RNN компьютер может понимать не просто отдельные слова, но смысл целых предложений.

==== Трансформеры (Transformers)

===== Революция механизма внимания

Трансформеры -- это совершенно новый способ работы с текстом. Вместо того чтобы читать слова одно за другим (как RNN), трансформер смотрит на *все слова сразу* и понимает, какие слова связаны между собой.

Представьте, что вы читаете предложение: "Каша гречневая очень вкусная". Трансформер сразу видит, что "каша" и "гречневая" связаны (гречневая каша), и "вкусная" относится к каше. Он не читает слово за словом, а схватывает всё вместе.

Эта новая идея назвали "механизм внимания" -- сеть автоматически определяет, на что нужно "обратить внимание", чтобы лучше понять текст.

===== Архитектура трансформера

Трансформер состоит из двух основных частей:

+ *Чтение текста (кодировщик):* сеть смотрит на входной текст и понимает его смысл. Она видит все слова сразу и определяет, какие слова важны друг для друга.

+ *Создание ответа (декодировщик):* сеть по одному генерирует слова ответа, каждый раз глядя на входной текст и на то, что она уже написала.

Ключевое отличие от RNN -- трансформер может обрабатывать текст *параллельно* (все слова сразу), а не последовательно (слово за словом), поэтому он работает намного быстрее.

==== Преимущества перед RNN

- *Скорость:* трансформер смотрит на весь текст одновременно, поэтому учится быстрее, чем RNN, который читает слово за словом.
- *Лучше запоминает длинный текст:* если в начале текста важная информация, трансформер не забудет её в конце, как иногда бывает с RNN.
- *Масштабируемость:* трансформеры легче научить на больших объёмах текста.

===== Современные модели на основе трансформеров

- *ChatGPT* -- знаменитая система, которая пишет тексты и отвечает на вопросы на английском и других языках
- *Google Translate* -- улучшенный перевод с использованием трансформеров
- *Системы для анализа текста* -- понимают тему, тональность (положительно или отрицательно), спам или нет
- *Системы для генерации картинок* -- могут рисовать изображения по описанию на словах
- *Голосовые помощники* -- улучшенное понимание команд пользователей

Все эти системы основаны на идее трансформеров.

===== Применение

Трансформеры сейчас используются практически везде, где нужно работать с текстом и информацией:

- *Помощь в учёбе:* система может объяснить сложную тему, помочь с домашним заданием, проверить эссе
- *Переводчик:* переводит с одного языка на другой с хорошим качеством
- *Помощник в написании:* может помочь структурировать ответ, подобрать синонимы, улучшить текст
- *Проверка понимания:* может создавать тесты и проверять ответы учеников
- *Генерация обучающего контента:* может придумать примеры задач, создать рабочие листы
- *Персональный репетитор:* может отвечать на вопросы ученика в любое время

Трансформеры делают образование более доступным и персональным.

=== Сравнение архитектур

#table(
  columns: (1fr, 1fr, 1fr, 1fr),
  [*Характеристика*], [*CNN*], [*RNN/LSTM*], [*Трансформер*],
  [Для чего?], [Для картинок], [Для текста и звука], [Для текста и других последовательностей],
  [Скорость], [Быстрая], [Медленнее], [Самая быстрая],
  [Запоминает ли контекст?], [Только локально], [Хорошо, но может забыть начало], [Отлично помнит весь текст],
  [Сложность], [Средняя], [Средняя], [Высокая],
  [Где используется?], [Распознавание фото], [Переводчики, голос], [ChatGPT, помощники],
)

=== Выводы

Мы рассмотрели три основных типа глубоких нейронных сетей: CNN для анализа изображений, RNN для работы с текстом и звуком, и новые трансформеры, которые работают как универсальные помощники. Каждая архитектура имеет свои плюсы и минусы, но все они вместе создают мощный набор инструментов для решения разных задач.

Для образования это особенно важно. Системы на основе трансформеров уже сейчас помогают ученикам получать персональные объяснения, проверку работ и мгновенную обратную связь. CNN помогает проверять письменные работы. RNN распознаёт речь учеников. Это означает, что в ближайшие годы каждый ученик сможет получить индивидуального цифрового помощника, который будет помогать ему учиться в его собственном темпе и стиле.
